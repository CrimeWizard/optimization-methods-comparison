# optimization-methods-comparison
Exploring how different optimizers (SGD, Momentum, RMSProp, Adam) perform on Fashion-MNIST using MLPs, with and without learning rate decay, and deploying the trained model as an API with FastAPI and Docker.
